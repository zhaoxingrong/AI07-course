{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 图像的Normalize\n",
    "\n",
    "每个像素-mean/std\n",
    "\n",
    "每个像素的归一化缩放\n",
    "\n",
    "思考:\n",
    "\n",
    "1.归一化哪部分数据？A训练集、B评测集、C训练集+评测集 -> C\n",
    "\n",
    "2.归一化的参数mean和std来自于？A训练集、B评测集、C训练集+评测集 -> A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.mean(mnist.train.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.std(mnist.train.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据的归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans=transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,),(0.3081,))#参数mean和std来自于训练集，但是transform本身在训练和评测的时候都会使用\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans_alexnet=transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,),(0.3081,))#参数mean和std来自于训练集，但是transform本身在训练和评测的时候都会使用\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data=datasets.MNIST('data',train=True,download=True,transform=data_trans)\n",
    "# test_data=datasets.MNIST('data',train=False,download=True,transform=data_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data=datasets.CIFAR10('data',train=True,download=True,transform=data_trans)\n",
    "test_data=datasets.CIFAR10('data',train=False,download=True,transform=data_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data=datasets.MNIST('data',train=True,download=True,transform=data_trans_alexnet)\n",
    "# test_data=datasets.MNIST('data',train=False,download=True,transform=data_trans_alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train=int(len(train_data)*0.9)\n",
    "n_validation=len(train_data)-n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,valid_data=torch.utils.data.random_split(train_data,[n_train,n_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45000"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Split: train\n",
       "    Root Location: data\n",
       "    Transforms (if any): Compose(\n",
       "                             Resize(size=32, interpolation=PIL.Image.BILINEAR)\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前完成了数据集的制作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator=torch.utils.data.DataLoader(train_data,shuffle=True,batch_size=batch_size)\n",
    "valid_iterator=torch.utils.data.DataLoader(valid_data,batch_size=batch_size)\n",
    "test_iterator=torch.utils.data.DataLoader(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet,self).__init__()\n",
    "        #第一层conv1，因为是MNIST数据集，所有channel数是1，输出的channel是6,kernel_size是5*5\n",
    "        self.conv1=nn.Conv2d(3,6,5)\n",
    "        #第二层conv2,输入channel=6,输出channel=16，kernel5*5,input_size=14*14,output_size=10*10\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        \n",
    "        self.fc1=nn.Linear(16*5*5,120)\n",
    "        \n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        \n",
    "        self.fc3=nn.Linear(84,10)#不用增加softmax层，从推断的角度直接使用argmax就可以得到最终的预测结果，在cross_entropy函数中实现了softmax的功能\n",
    "        \n",
    "    def forward(self,x):#规定计算图架构\n",
    "        out=F.max_pool2d(F.relu(self.conv1(x)),2)\n",
    "        out=F.max_pool2d(F.relu(self.conv2(out)),2)\n",
    "        out=out.view(out.shape[0],-1)\n",
    "        out=F.relu(self.fc1(out))\n",
    "        out=F.relu(self.fc2(out))\n",
    "        out=self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet,self).__init__()\n",
    "        self.feature_block=nn.Sequential(\n",
    "            nn.Conv2d(3,64,kernel_size=11,stride=4,padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "            nn.Conv2d(64,192,kernel_size=5,padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "            nn.Conv2d(192,384,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384,256,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "        )\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d((6,6))\n",
    "        self.class_block=nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(6*6*256,4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096,10),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x=self.feature_block(x)\n",
    "        x=self.avgpool(x)\n",
    "        x=x.view(x.size(0),256*6*6)\n",
    "        x=self.class_block(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBlock(nn.Module):\n",
    "    def __init__(self,in_channel,out_channel,batch_norm):#改良后的新的VGGBlock\n",
    "        super(VGGBlock,self).__init__()\n",
    "        stack=[]\n",
    "        stack.append(nn.Conv2d(in_channel,out_channel,kernel_size=3,padding=1))\n",
    "        if batch_norm:\n",
    "            stack.append(nn.BatchNorm2d(out_channel))\n",
    "        stack.append(nn.ReLU(inplace=True))\n",
    "        self.model_block=nn.Sequential(*stack)\n",
    "    def forward(self,x):\n",
    "        return self.model_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet11(nn.Module):\n",
    "    def __init__(self,block,pool,batch_norm):#block是一个网络模组抽象，pool也是pooling层的抽象\n",
    "        super(VGGNet11,self).__init__()\n",
    "        self.feature_block=nn.Sequential(\n",
    "            block(3,64,batch_norm), #32*32\n",
    "            pool(kernel_size=2,stride=2),#16*16\n",
    "            block(64,128,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#8*8\n",
    "            block(128,256,batch_norm),\n",
    "            block(256,256,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#4*4\n",
    "            block(256,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#2*2\n",
    "            block(512,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#1*1\n",
    "        )\n",
    "        self.classifier=nn.Linear(512,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.feature_block(x)\n",
    "        x=x.view(x.shape[0],-1)\n",
    "        x=self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet16(nn.Module):\n",
    "    def __init__(self,block,pool,batch_norm):#block是一个网络模组抽象，pool也是pooling层的抽象\n",
    "        super(VGGNet16,self).__init__()\n",
    "        self.feature_block=nn.Sequential(\n",
    "            block(3,64,batch_norm), #32*32\n",
    "            block(64,64,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#16*16\n",
    "            block(64,128,batch_norm),\n",
    "            block(128,128,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#8*8\n",
    "            block(128,256,batch_norm),\n",
    "            block(256,256,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#4*4\n",
    "            block(256,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#2*2\n",
    "            block(512,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#1*1\n",
    "        )\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.Linear(512,1024),\n",
    "            nn.Linear(1024,1024),\n",
    "            nn.Linear(1024,10),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.feature_block(x)\n",
    "        x=x.view(x.shape[0],-1)\n",
    "        x=self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self,in_planes,n1x1,n3x3red,n3x3,n5x5red,n5x5,pool_planes):\n",
    "        super(Inception,self).__init__()\n",
    "        self.b1=nn.Sequential(\n",
    "            nn.Conv2d(in_planes,n1x1,kernel_size=1),\n",
    "            nn.BatchNorm2d(n1x1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        self.b2=nn.Sequential(\n",
    "            nn.Conv2d(in_planes,n3x3red,kernel_size=1),\n",
    "            nn.BatchNorm2d(n3x3red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n3x3red,n3x3,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(n3x3),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        self.b3=nn.Sequential(\n",
    "            nn.Conv2d(in_planes,n5x5red,kernel_size=1),\n",
    "            nn.BatchNorm2d(n5x5red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n5x5red,n5x5,kernel_size=5,padding=2),\n",
    "            nn.BatchNorm2d(n5x5),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        self.b4=nn.Sequential(\n",
    "            nn.MaxPool2d(3,stride=1,padding=1),\n",
    "            nn.Conv2d(in_planes,pool_planes,kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_planes),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x1=self.b1(x)\n",
    "        x2=self.b2(x)\n",
    "        x3=self.b3(x)\n",
    "        x4=self.b4(x)\n",
    "        #concat4层输入在一起\n",
    "        return torch.cat([x1,x2,x3,x4],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GoogLeNet,self).__init__()\n",
    "        self.feature_block=nn.Sequential(\n",
    "            nn.Conv2d(3,192,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.a3=Inception(192,64,96,128,16,32,32)\n",
    "        self.b3=Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool=nn.MaxPool2d(3,stride=2,padding=1)\n",
    "        self.a4 = Inception(480, 192,  96, 208, 16,  48,  64)\n",
    "        self.b4 = Inception(512, 160, 112, 224, 24,  64,  64)\n",
    "        self.c4 = Inception(512, 128, 128, 256, 24,  64,  64)\n",
    "        self.d4 = Inception(512, 112, 144, 288, 32,  64,  64)\n",
    "        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "        self.avgpool=nn.AvgPool2d(8,stride=1)\n",
    "        self.linear=nn.Linear(1024,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.feature_block(x)\n",
    "        out=self.a3(out)\n",
    "        out=self.b3(out)\n",
    "        out=self.maxpool(out)\n",
    "        out=self.a4(out)\n",
    "        out=self.b4(out)\n",
    "        out=self.c4(out)\n",
    "        out=self.d4(out)\n",
    "        out=self.e4(out)\n",
    "        out=self.maxpool(out)\n",
    "        out = self.a5(out)\n",
    "        out = self.b5(out)\n",
    "        out =self.avgpool(out)\n",
    "        out =out.view(out.size(0),-1)\n",
    "        out=self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(ResNetBlock,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = nn.Sequential()\n",
    "        \n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.downsample(x)#ResNet的add操作，其实是张量的加和\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNetLayer(nn.Module):\n",
    "    def __init__(self,block,n_blocks,in_channels,out_channels,stride):\n",
    "        super(ResNetLayer,self).__init__()\n",
    "        self.modules=[]\n",
    "        self.modules.append(block(in_channels,out_channels,stride))\n",
    "        for _ in range(n_blocks-1):\n",
    "            self.modules.append(block(out_channels,out_channels,1))\n",
    "        self.blocks=nn.Sequential(*self.modules)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.blocks(x)\n",
    "    \n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self,layer,block):\n",
    "        super(ResNet18,self).__init__()\n",
    "        n_blocks=[2,2,2,2]\n",
    "        self.conv1=nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.bn1=nn.BatchNorm2d(64)\n",
    "        self.rb1=layer(block,n_blocks[0],64,64,1)\n",
    "        self.rb2=layer(block,n_blocks[1],64,128,2)\n",
    "        self.rb3=layer(block,n_blocks[2],128,256,2)\n",
    "        self.rb4=layer(block,n_blocks[3],256,512,2)\n",
    "        self.fc=nn.Linear(512,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=F.relu(self.bn1(self.conv1(x)))\n",
    "        out=self.rb1(out)\n",
    "        out=self.rb2(out)\n",
    "        out=self.rb3(out)\n",
    "        out=self.rb4(out)\n",
    "        out=F.avg_pool2d(out,4)\n",
    "        out=out.view(out.shape[0],-1)\n",
    "        out=self.fc(out)\n",
    "        return out\n",
    "        \n",
    "#ResNet34->[3,4,6,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self,in_planes,growth_rate):\n",
    "        super(Bottleneck,self).__init__()\n",
    "        self.bn1=nn.BatchNorm2d(in_planes)\n",
    "        self.conv1=nn.Conv2d(in_planes,4*growth_rate,kernel_size=1,bias=False)\n",
    "        self.bn2=nn.BatchNorm2d(4*growth_rate)\n",
    "        self.conv2=nn.Conv2d(4*growth_rate,growth_rate,kernel_size=3,padding=1,bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.conv1(F.relu(self.bn1(x)))#pre-activation\n",
    "        out=self.conv2(F.relu(self.bn2(out)))\n",
    "        out=torch.cat([out,x],1)\n",
    "        return out\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self,in_planes,out_planes):\n",
    "        super(Transition,self).__init__()\n",
    "        self.bn=nn.BatchNorm2d(in_planes)\n",
    "        self.conv=nn.Conv2d(in_planes,out_planes,kernel_size=1,bias=False)\n",
    "    def forward(self,x):\n",
    "        out=self.conv(F.relu(self.bn(x)))\n",
    "        out=F.avg_pool2d(out,2)\n",
    "        return out\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self,block,nblocks,growth_rate=12,reduction=0.5,num_classes=10):\n",
    "        super(DenseNet,self).__init__()\n",
    "        self.growth_rate=growth_rate\n",
    "        num_planes=2*growth_rate #32\n",
    "        #最初的感知层\n",
    "        self.conv1=nn.Conv2d(3,num_planes,kernel_size=3,padding=1,bias=False)\n",
    "        #第一个DenseBlock\n",
    "        self.dense1=self._make_dense_layers(block,num_planes,nblocks[0])\n",
    "        num_planes+=nblocks[0]*growth_rate\n",
    "        out_planes=int(math.floor(num_planes*reduction))\n",
    "        self.trans1=Transition(num_planes,out_planes)\n",
    "        num_planes=out_planes\n",
    "        #第二个DenseBlock\n",
    "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
    "        num_planes += nblocks[1]*growth_rate#计算如果不压缩的话的输出\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans2 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "        #第三个DenseBlock\n",
    "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
    "        num_planes += nblocks[2]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans3 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "        #第四个DenseBlock\n",
    "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
    "        num_planes += nblocks[3]*growth_rate\n",
    "        #分类层\n",
    "        self.bn=nn.BatchNorm2d(num_planes)\n",
    "        self.linear=nn.Linear(num_planes,num_classes)\n",
    "    \n",
    "    \n",
    "    def _make_dense_layers(self,block,in_planes,nblock):\n",
    "        #block:bottleneck\n",
    "        #nblock代表构建denseblock中有多少bottleneck层\n",
    "        layers=[]\n",
    "        for i in range(nblock):\n",
    "            layers.append(block(in_planes,self.growth_rate))\n",
    "            in_planes+=self.growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out=self.conv1(x)\n",
    "        out=self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out=F.avg_pool2d(F.relu(self.bn(out)),4)\n",
    "        out=out.view(out.size(0),-1)\n",
    "        out=self.linear(out)\n",
    "        return out\n",
    "\n",
    "def DenseNet121():\n",
    "    return DenseNet(Bottleneck,[6,12,24,16],growth_rate=32)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    expansion = 2\n",
    "\n",
    "    def __init__(self, in_planes, cardinality=32, bottleneck_width=4, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        group_width = cardinality * bottleneck_width\n",
    "        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(group_width)\n",
    "        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(group_width)\n",
    "        self.conv3 = nn.Conv2d(group_width, self.expansion*group_width, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*group_width)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*group_width:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*group_width, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*group_width)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNeXt(nn.Module):\n",
    "    def __init__(self, num_blocks, cardinality, bottleneck_width, num_classes=10):\n",
    "        super(ResNeXt, self).__init__()\n",
    "        self.cardinality = cardinality\n",
    "        self.bottleneck_width = bottleneck_width\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(num_blocks[0], 1)\n",
    "        self.layer2 = self._make_layer(num_blocks[1], 2)\n",
    "        self.layer3 = self._make_layer(num_blocks[2], 2)\n",
    "        # self.layer4 = self._make_layer(num_blocks[3], 2)\n",
    "        self.linear = nn.Linear(cardinality*bottleneck_width*8, num_classes)\n",
    "\n",
    "    def _make_layer(self, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(Block(self.in_planes, self.cardinality, self.bottleneck_width, stride))\n",
    "            self.in_planes = Block.expansion * self.cardinality * self.bottleneck_width\n",
    "        # Increase bottleneck_width by 2 after each stage.\n",
    "        self.bottleneck_width *= 2\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNeXt29_2x64d():\n",
    "    return ResNeXt(num_blocks=[3,3,3], cardinality=2, bottleneck_width=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "class Fire(nn.Module):\n",
    "    def __init__(self,inplanes,s1,e1,e3):\n",
    "        super(Fire,self).__init__()\n",
    "        self.inplanes=inplanes\n",
    "        self.squeeze=nn.Conv2d(inplanes,s1,kernel_size=1)\n",
    "        self.squeeze_activation=nn.ReLU(inplace=True)\n",
    "        self.expand1x1=nn.Conv2d(s1,e1,kernel_size=1)\n",
    "        self.expand1x1_activation=nn.ReLU(inplace=True)\n",
    "        self.expand3x3=nn.Conv2d(s1,e3,kernel_size=True)\n",
    "        self.expand3x3_activation=nn.ReLU(inplace=True)\n",
    "    def forward(self,x):\n",
    "        x=self.squeeze_activation(self.squeeze(x))\n",
    "        return torch.cat([\n",
    "            self.expand1x1_activation(self.expand1x1(x)),\n",
    "            self.expand3x3_activation(self.expand3x3(x))\n",
    "        ],1)\n",
    "    \n",
    "class SqueezeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, version=1.0, num_classes=10):\n",
    "        super(SqueezeNet, self).__init__()\n",
    "        if version not in [1.0, 1.1]:\n",
    "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
    "                             \"1.0 or 1.1 expected\".format(version=version))\n",
    "        self.num_classes = num_classes\n",
    "        if version == 1.0:\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(96, 16, 64, 64),\n",
    "                Fire(128, 16, 64, 64),\n",
    "                Fire(128, 32, 128, 128),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(256, 32, 128, 128),\n",
    "                Fire(256, 48, 192, 192),\n",
    "                Fire(384, 48, 192, 192),\n",
    "                Fire(384, 64, 256, 256),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(512, 64, 256, 256),\n",
    "            )\n",
    "        else:\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(64, 16, 64, 64),\n",
    "                Fire(128, 16, 64, 64),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(128, 32, 128, 128),\n",
    "                Fire(256, 32, 128, 128),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(256, 48, 192, 192),\n",
    "                Fire(384, 48, 192, 192),\n",
    "                Fire(384, 64, 256, 256),\n",
    "                Fire(512, 64, 256, 256),\n",
    "            )\n",
    "        # Final convolution is initialized differently form the rest\n",
    "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            #nn.Dropout(p=0.5),\n",
    "            final_conv,\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.AvgPool2d(4, stride=1)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if m is final_conv:\n",
    "                    init.normal(m.weight.data, mean=0.0, std=0.01)\n",
    "                else:\n",
    "                    init.kaiming_uniform(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x.view(x.size(0), self.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"DWConv+PWConv\"\n",
    "    def __init__(self,in_planes,out_planes,stride):\n",
    "        super(Block,self).__init__()\n",
    "        self.conv1=nn.Conv2d(in_planes,in_planes,kernel_size=3,stride=stride,padding=1,groups=in_planes,bias=False)\n",
    "        self.bn1=nn.BatchNorm2d(in_planes)\n",
    "        self.conv2=nn.Conv2d(in_planes,out_planes,kernel_size=1,stride=1,padding=0,bias=False)\n",
    "        self.bn2=nn.BatchNorm2d(out_planes)\n",
    "    def forward(self,x):\n",
    "        out=F.relu(self.bn1(self.conv1(x)))\n",
    "        out=F.relu(self.bn2(self.conv2(out)))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MobileNet(nn.Module):\n",
    "    cfg=[64, (128,2), 128, (256,2), 256, (512,2), 512, 512, 512, 512, 512, (1024,2), 1024]\n",
    "    def __init__(self,num_classes=10):\n",
    "        super(MobileNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.layers=self._make_layers(in_planes=32)\n",
    "        self.linear=nn.Linear(1024,num_classes)\n",
    "        \n",
    "    def _make_layers(self,in_planes):\n",
    "        layers=[]\n",
    "        for x in self.cfg:\n",
    "            out_planes=x if isinstance(x,int) else x[0]\n",
    "            stride=1 if isinstance(x,int) else x[1]\n",
    "            layers.append(Block(in_planes,out_planes,stride))\n",
    "            in_planes=out_planes\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self,x):\n",
    "        out=F.relu(self.bn1(self.conv1(x)))\n",
    "        out=self.layers(out)\n",
    "        out=F.avg_pool2d(out,2)\n",
    "        out=out.view(out.size(0),-1)\n",
    "        out=self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ShuffleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cfg={\n",
    "    'out_planes':[200,400,800],\n",
    "    'num_blocks':[4,8,4],\n",
    "    'groups':2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, groups):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        self.stride = stride\n",
    "\n",
    "        mid_planes = out_planes/4\n",
    "        g = 1 if in_planes==24 else groups\n",
    "        self.conv1 = nn.Conv2d(in_planes, mid_planes, kernel_size=1, groups=g, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(mid_planes)\n",
    "        self.shuffle1 = ShuffleBlock(groups=g)\n",
    "        self.conv2 = nn.Conv2d(mid_planes, mid_planes, kernel_size=3, stride=stride, padding=1, groups=mid_planes, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(mid_planes)\n",
    "        self.conv3 = nn.Conv2d(mid_planes, out_planes, kernel_size=1, groups=groups, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride == 2:\n",
    "            self.shortcut = nn.Sequential(nn.AvgPool2d(3, stride=2, padding=1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.shuffle1(out)\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        res = self.shortcut(x)\n",
    "        out = F.relu(torch.cat([out,res], 1)) if self.stride==2 else F.relu(out+res)\n",
    "        return out\n",
    "    \n",
    "class ShuffleNet(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(ShuffleNet, self).__init__()\n",
    "        out_planes = cfg['out_planes']\n",
    "        num_blocks = cfg['num_blocks']\n",
    "        groups = cfg['groups']\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 24, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(24)\n",
    "        self.in_planes = 24\n",
    "        self.layer1 = self._make_layer(out_planes[0], num_blocks[0], groups)\n",
    "        self.layer2 = self._make_layer(out_planes[1], num_blocks[1], groups)\n",
    "        self.layer3 = self._make_layer(out_planes[2], num_blocks[2], groups)\n",
    "        self.linear = nn.Linear(out_planes[2], 10)\n",
    "\n",
    "    def _make_layer(self, out_planes, num_blocks, groups):\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            stride = 2 if i == 0 else 1\n",
    "            cat_planes = self.in_planes if i == 0 else 0\n",
    "            layers.append(Bottleneck(self.in_planes, out_planes-cat_planes, stride=stride, groups=groups))\n",
    "            self.in_planes = out_planes\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 载入模型并进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir='models'\n",
    "\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=LeNet().to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'lenet_mnist.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=AlexNet().to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'alex_mnist.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=VGGNet11(VGGBlock,nn.MaxPool2d,True).to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'vgg_mnist.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=VGGNet16(VGGBlock,nn.MaxPool2d,True).to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'vgg16_mnist.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=GoogLeNet().to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'googlenet_mnist.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=ResNet18(ResNetLayer,ResNetBlock).to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'resnet18_mnist.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=DenseNet121().to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'densenet121_mnist.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=ResNeXt29_2x64d().to(device)\n",
    "# model_path=os.path.join(model_dir,'resnext_29_2x64d_mnist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=LeNet().to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'lenet_cifar10.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=ResNet18(ResNetLayer,ResNetBlock).to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'resnet18_cifar10.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=SqueezeNet(version=1.0).to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'squeezenet_cifar10.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=MobileNet().to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'mobilenet_cifar10.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg={\n",
    "    'out_planes':[200,400,800],\n",
    "    'num_blocks':[4,8,4],\n",
    "    'groups':2\n",
    "}\n",
    "model=ShuffleNet(cfg).to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "model_path=os.path.join(model_dir,'shufflenet_cifar10.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShuffleNet(\n",
       "  (conv1): Conv2d(3, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(24, 44, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shuffle1): ShuffleBlock()\n",
       "      (conv2): Conv2d(44, 44, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=44, bias=False)\n",
       "      (bn2): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(44, 176, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn3): BatchNorm2d(176, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(200, 50, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shuffle1): ShuffleBlock()\n",
       "      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=50, bias=False)\n",
       "      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(50, 200, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn3): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(200, 50, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shuffle1): ShuffleBlock()\n",
       "      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=50, bias=False)\n",
       "      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(50, 200, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn3): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(200, 50, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shuffle1): ShuffleBlock()\n",
       "      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=50, bias=False)\n",
       "      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(50, 200, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn3): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(200, 50, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn1): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shuffle1): ShuffleBlock()\n",
       "      (conv2): Conv2d(50, 50, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=50, bias=False)\n",
       "      (bn2): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(50, 200, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn3): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(400, 100, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shuffle1): ShuffleBlock()\n",
       "      (conv2): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
       "      (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(100, 400, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(400, 100, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shuffle1): ShuffleBlock()\n",
       "      (conv2): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
       "      (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(100, 400, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(400, 100, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shuffle1): ShuffleBlock()\n",
       "      (conv2): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
       "      (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(100, 400, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(400, 100, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shuffle1): ShuffleBlock()\n",
       "      (conv2): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
       "      (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(100, 400, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(400, 100, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shuffle1): ShuffleBlock()\n",
       "      (conv2): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
       "      (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(100, 400, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(400, 100, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shuffle1): ShuffleBlock()\n",
       "      (conv2): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
       "      (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(100, 400, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(400, 100, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shuffle1): ShuffleBlock()\n",
       "      (conv2): Conv2d(100, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=100, bias=False)\n",
       "      (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(100, 400, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(400, 100, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shuffle1): ShuffleBlock()\n",
       "      (conv2): Conv2d(100, 100, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=100, bias=False)\n",
       "      (bn2): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(100, 400, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn3): BatchNorm2d(400, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential(\n",
       "        (0): AvgPool2d(kernel_size=3, stride=2, padding=1)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(800, 200, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shuffle1): ShuffleBlock()\n",
       "      (conv2): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "      (bn2): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(200, 800, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn3): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(800, 200, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shuffle1): ShuffleBlock()\n",
       "      (conv2): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "      (bn2): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(200, 800, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn3): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(800, 200, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn1): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shuffle1): ShuffleBlock()\n",
       "      (conv2): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=200, bias=False)\n",
       "      (bn2): BatchNorm2d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(200, 800, kernel_size=(1, 1), stride=(1, 1), groups=2, bias=False)\n",
       "      (bn3): BatchNorm2d(800, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (linear): Linear(in_features=800, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accu(fx,y):\n",
    "    pred=fx.max(1,keepdim=True)[1]\n",
    "    correct=pred.eq(y.view_as(pred)).sum()\n",
    "    acc=correct.float()/pred.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练一个epoch\n",
    "def train(model,device,iterator,optimizer,criterion):\n",
    "    epoch_loss=0#积累变量\n",
    "    epoch_acc=0#积累变量\n",
    "    model.train()#该函数表示PHASE=Train,自动求导以及求导运算将会被激活\n",
    "    \n",
    "    for (x,y) in iterator:#拿每一个minibatch\n",
    "#         print(x,y)\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        optimizer.zero_grad()#将所有的梯度变量清零\n",
    "        fx=model(x)#进行inference\n",
    "        loss=criterion(fx,y)#计算train_loss\n",
    "        acc=accu(fx,y)#计算train_acc\n",
    "        loss.backward()#进行bp回算各参数和神经元的梯度\n",
    "        optimizer.step()#统一进行梯度下降的更新\n",
    "        epoch_loss+=loss.item()\n",
    "        epoch_acc+=acc.item()\n",
    "    \n",
    "    #返回平均训练Loss和平均训练Accu\n",
    "    return epoch_loss/len(iterator),epoch_acc/len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "#评测一个验证集，不用梯度下降，只是进行推断和误差计算\n",
    "def evaluate(model,device,iterator,criterion):\n",
    "    epoch_loss=0\n",
    "    epoch_acc=0\n",
    "    model.eval()#PHASE=Eval,不会增加梯度的存储变量和计算单元\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in iterator:\n",
    "            x=x.to(device)\n",
    "        \n",
    "            y=y.to(device)\n",
    "            fx=model(x)\n",
    "            loss=criterion(fx,y)\n",
    "            acc=accu(fx,y)\n",
    "            epoch_loss+=loss.item()\n",
    "            epoch_acc+=acc.item()\n",
    "            \n",
    "    return epoch_loss/len(iterator),epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss=float('inf')#自动筛选最优模型并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1|Train Loss:1.54935040994|Train Acc:0.426602450284|Val Loss:1.30605506293|Val Acc:0.515625\n",
      "Epoch:2|Train Loss:1.11998806496|Train Acc:0.602938565341|Val Loss:1.01534796214|Val Acc:0.645569620253\n",
      "Epoch:3|Train Loss:0.912617150144|Train Acc:0.676669034091|Val Loss:0.887096593652|Val Acc:0.681962025316\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss,train_acc=train(model,device,train_iterator,optimizer,criterion)\n",
    "    valid_loss,valid_acc=evaluate(model,device,valid_iterator,criterion)\n",
    "    if valid_loss<best_valid_loss:#当前模型好于历史最好模型\n",
    "        best_valid_loss=valid_loss\n",
    "        torch.save(model.state_dict(),model_path)#模型文件的更新\n",
    "    print('Epoch:{0}|Train Loss:{1}|Train Acc:{2}|Val Loss:{3}|Val Acc:{4}'.format(epoch+1,train_loss,train_acc,valid_loss,valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path))#load最好结果到model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss,test_acc=evaluate(model,device,test_iterator,criterion)\n",
    "print('Test Loss:{0}|Test Acc:{1}'.format(test_loss,test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作业:\n",
    "\n",
    "请大家实现ResNet50/101/152->只需要成功的跑起来即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
