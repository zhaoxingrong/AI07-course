{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import random\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 图像的Normalize\n",
    "\n",
    "每个像素-mean/std\n",
    "\n",
    "每个像素的归一化缩放\n",
    "\n",
    "思考:\n",
    "\n",
    "1.归一化哪部分数据？A训练集、B评测集、C训练集+评测集 -> C\n",
    "\n",
    "2.归一化的参数mean和std来自于？A训练集、B评测集、C训练集+评测集 -> A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.mean(mnist.train.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.std(mnist.train.images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 数据的归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans=transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,),(0.3081,))#参数mean和std来自于训练集，但是transform本身在训练和评测的时候都会使用\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans_alexnet=transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,),(0.3081,))#参数mean和std来自于训练集，但是transform本身在训练和评测的时候都会使用\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data=datasets.MNIST('data',train=True,download=True,transform=data_trans)\n",
    "# test_data=datasets.MNIST('data',train=False,download=True,transform=data_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "train_data=datasets.CIFAR10('data',train=True,download=True,transform=data_trans)\n",
    "test_data=datasets.CIFAR10('data',train=False,download=True,transform=data_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data=datasets.MNIST('data',train=True,download=True,transform=data_trans_alexnet)\n",
    "# test_data=datasets.MNIST('data',train=False,download=True,transform=data_trans_alexnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train=int(len(train_data)*0.9)\n",
    "n_validation=len(train_data)-n_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,valid_data=torch.utils.data.random_split(train_data,[n_train,n_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45000"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Split: train\n",
       "    Root Location: data\n",
       "    Transforms (if any): Compose(\n",
       "                             Resize(size=32, interpolation=PIL.Image.BILINEAR)\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "目前完成了数据集的制作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator=torch.utils.data.DataLoader(train_data,shuffle=True,batch_size=batch_size)\n",
    "valid_iterator=torch.utils.data.DataLoader(valid_data,batch_size=batch_size)\n",
    "test_iterator=torch.utils.data.DataLoader(test_data,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "704"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 构建神经网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet,self).__init__()\n",
    "        #第一层conv1，因为是MNIST数据集，所有channel数是1，输出的channel是6,kernel_size是5*5\n",
    "        self.conv1=nn.Conv2d(3,6,5)\n",
    "        #第二层conv2,输入channel=6,输出channel=16，kernel5*5,input_size=14*14,output_size=10*10\n",
    "        self.conv2=nn.Conv2d(6,16,5)\n",
    "        \n",
    "        self.fc1=nn.Linear(16*5*5,120)\n",
    "        \n",
    "        self.fc2=nn.Linear(120,84)\n",
    "        \n",
    "        self.fc3=nn.Linear(84,10)#不用增加softmax层，从推断的角度直接使用argmax就可以得到最终的预测结果，在cross_entropy函数中实现了softmax的功能\n",
    "        \n",
    "    def forward(self,x):#规定计算图架构\n",
    "        out=F.max_pool2d(F.relu(self.conv1(x)),2)\n",
    "        out=F.max_pool2d(F.relu(self.conv2(out)),2)\n",
    "        out=out.view(out.shape[0],-1)\n",
    "        out=F.relu(self.fc1(out))\n",
    "        out=F.relu(self.fc2(out))\n",
    "        out=self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet,self).__init__()\n",
    "        self.feature_block=nn.Sequential(\n",
    "            nn.Conv2d(3,64,kernel_size=11,stride=4,padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "            nn.Conv2d(64,192,kernel_size=5,padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "            nn.Conv2d(192,384,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384,256,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256,256,kernel_size=3,padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "        )\n",
    "        self.avgpool=nn.AdaptiveAvgPool2d((6,6))\n",
    "        self.class_block=nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(6*6*256,4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096,4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096,10),\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        x=self.feature_block(x)\n",
    "        x=self.avgpool(x)\n",
    "        x=x.view(x.size(0),256*6*6)\n",
    "        x=self.class_block(x)\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGBlock(nn.Module):\n",
    "    def __init__(self,in_channel,out_channel,batch_norm):#改良后的新的VGGBlock\n",
    "        super(VGGBlock,self).__init__()\n",
    "        stack=[]\n",
    "        stack.append(nn.Conv2d(in_channel,out_channel,kernel_size=3,padding=1))\n",
    "        if batch_norm:\n",
    "            stack.append(nn.BatchNorm2d(out_channel))\n",
    "        stack.append(nn.ReLU(inplace=True))\n",
    "        self.model_block=nn.Sequential(*stack)\n",
    "    def forward(self,x):\n",
    "        return self.model_block(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet11(nn.Module):\n",
    "    def __init__(self,block,pool,batch_norm):#block是一个网络模组抽象，pool也是pooling层的抽象\n",
    "        super(VGGNet11,self).__init__()\n",
    "        self.feature_block=nn.Sequential(\n",
    "            block(3,64,batch_norm), #32*32\n",
    "            pool(kernel_size=2,stride=2),#16*16\n",
    "            block(64,128,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#8*8\n",
    "            block(128,256,batch_norm),\n",
    "            block(256,256,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#4*4\n",
    "            block(256,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#2*2\n",
    "            block(512,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#1*1\n",
    "        )\n",
    "        self.classifier=nn.Linear(512,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.feature_block(x)\n",
    "        x=x.view(x.shape[0],-1)\n",
    "        x=self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGGNet16(nn.Module):\n",
    "    def __init__(self,block,pool,batch_norm):#block是一个网络模组抽象，pool也是pooling层的抽象\n",
    "        super(VGGNet16,self).__init__()\n",
    "        self.feature_block=nn.Sequential(\n",
    "            block(3,64,batch_norm), #32*32\n",
    "            block(64,64,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#16*16\n",
    "            block(64,128,batch_norm),\n",
    "            block(128,128,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#8*8\n",
    "            block(128,256,batch_norm),\n",
    "            block(256,256,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#4*4\n",
    "            block(256,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#2*2\n",
    "            block(512,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            block(512,512,batch_norm),\n",
    "            pool(kernel_size=2,stride=2),#1*1\n",
    "        )\n",
    "        self.classifier=nn.Sequential(\n",
    "            nn.Linear(512,1024),\n",
    "            nn.Linear(1024,1024),\n",
    "            nn.Linear(1024,10),\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.feature_block(x)\n",
    "        x=x.view(x.shape[0],-1)\n",
    "        x=self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GoogleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(nn.Module):\n",
    "    def __init__(self,in_planes,n1x1,n3x3red,n3x3,n5x5red,n5x5,pool_planes):\n",
    "        super(Inception,self).__init__()\n",
    "        self.b1=nn.Sequential(\n",
    "            nn.Conv2d(in_planes,n1x1,kernel_size=1),\n",
    "            nn.BatchNorm2d(n1x1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        self.b2=nn.Sequential(\n",
    "            nn.Conv2d(in_planes,n3x3red,kernel_size=1),\n",
    "            nn.BatchNorm2d(n3x3red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n3x3red,n3x3,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(n3x3),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        self.b3=nn.Sequential(\n",
    "            nn.Conv2d(in_planes,n5x5red,kernel_size=1),\n",
    "            nn.BatchNorm2d(n5x5red),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(n5x5red,n5x5,kernel_size=5,padding=2),\n",
    "            nn.BatchNorm2d(n5x5),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        self.b4=nn.Sequential(\n",
    "            nn.MaxPool2d(3,stride=1,padding=1),\n",
    "            nn.Conv2d(in_planes,pool_planes,kernel_size=1),\n",
    "            nn.BatchNorm2d(pool_planes),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x1=self.b1(x)\n",
    "        x2=self.b2(x)\n",
    "        x3=self.b3(x)\n",
    "        x4=self.b4(x)\n",
    "        #concat4层输入在一起\n",
    "        return torch.cat([x1,x2,x3,x4],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoogLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GoogLeNet,self).__init__()\n",
    "        self.feature_block=nn.Sequential(\n",
    "            nn.Conv2d(3,192,kernel_size=3,padding=1),\n",
    "            nn.BatchNorm2d(192),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        self.a3=Inception(192,64,96,128,16,32,32)\n",
    "        self.b3=Inception(256, 128, 128, 192, 32, 96, 64)\n",
    "        self.maxpool=nn.MaxPool2d(3,stride=2,padding=1)\n",
    "        self.a4 = Inception(480, 192,  96, 208, 16,  48,  64)\n",
    "        self.b4 = Inception(512, 160, 112, 224, 24,  64,  64)\n",
    "        self.c4 = Inception(512, 128, 128, 256, 24,  64,  64)\n",
    "        self.d4 = Inception(512, 112, 144, 288, 32,  64,  64)\n",
    "        self.e4 = Inception(528, 256, 160, 320, 32, 128, 128)\n",
    "        self.a5 = Inception(832, 256, 160, 320, 32, 128, 128)\n",
    "        self.b5 = Inception(832, 384, 192, 384, 48, 128, 128)\n",
    "        self.avgpool=nn.AvgPool2d(8,stride=1)\n",
    "        self.linear=nn.Linear(1024,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.feature_block(x)\n",
    "        out=self.a3(out)\n",
    "        out=self.b3(out)\n",
    "        out=self.maxpool(out)\n",
    "        out=self.a4(out)\n",
    "        out=self.b4(out)\n",
    "        out=self.c4(out)\n",
    "        out=self.d4(out)\n",
    "        out=self.e4(out)\n",
    "        out=self.maxpool(out)\n",
    "        out = self.a5(out)\n",
    "        out = self.b5(out)\n",
    "        out =self.avgpool(out)\n",
    "        out =out.view(out.size(0),-1)\n",
    "        out=self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super(ResNetBlock,self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = nn.Sequential()\n",
    "        \n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.downsample(x)#ResNet的add操作，其实是张量的加和\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "class ResNetLayer(nn.Module):\n",
    "    def __init__(self,block,n_blocks,in_channels,out_channels,stride):\n",
    "        super(ResNetLayer,self).__init__()\n",
    "        self.modules=[]\n",
    "        self.modules.append(block(in_channels,out_channels,stride))\n",
    "        for _ in range(n_blocks-1):\n",
    "            self.modules.append(block(out_channels,out_channels,1))\n",
    "        self.blocks=nn.Sequential(*self.modules)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.blocks(x)\n",
    "    \n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self,layer,block):\n",
    "        super(ResNet18,self).__init__()\n",
    "        n_blocks=[2,2,2,2]\n",
    "        self.conv1=nn.Conv2d(3,64,kernel_size=3,stride=1,padding=1,bias=False)\n",
    "        self.bn1=nn.BatchNorm2d(64)\n",
    "        self.rb1=layer(block,n_blocks[0],64,64,1)\n",
    "        self.rb2=layer(block,n_blocks[1],64,128,2)\n",
    "        self.rb3=layer(block,n_blocks[2],128,256,2)\n",
    "        self.rb4=layer(block,n_blocks[3],256,512,2)\n",
    "        self.fc=nn.Linear(512,10)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=F.relu(self.bn1(self.conv1(x)))\n",
    "        out=self.rb1(out)\n",
    "        out=self.rb2(out)\n",
    "        out=self.rb3(out)\n",
    "        out=self.rb4(out)\n",
    "        out=F.avg_pool2d(out,4)\n",
    "        out=out.view(out.shape[0],-1)\n",
    "        out=self.fc(out)\n",
    "        return out\n",
    "        \n",
    "#ResNet34->[3,4,6,3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self,in_planes,growth_rate):\n",
    "        super(Bottleneck,self).__init__()\n",
    "        self.bn1=nn.BatchNorm2d(in_planes)\n",
    "        self.conv1=nn.Conv2d(in_planes,4*growth_rate,kernel_size=1,bias=False)\n",
    "        self.bn2=nn.BatchNorm2d(4*growth_rate)\n",
    "        self.conv2=nn.Conv2d(4*growth_rate,growth_rate,kernel_size=3,padding=1,bias=False)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out=self.conv1(F.relu(self.bn1(x)))#pre-activation\n",
    "        out=self.conv2(F.relu(self.bn2(out)))\n",
    "        out=torch.cat([out,x],1)\n",
    "        return out\n",
    "class Transition(nn.Module):\n",
    "    def __init__(self,in_planes,out_planes):\n",
    "        super(Transition,self).__init__()\n",
    "        self.bn=nn.BatchNorm2d(in_planes)\n",
    "        self.conv=nn.Conv2d(in_planes,out_planes,kernel_size=1,bias=False)\n",
    "    def forward(self,x):\n",
    "        out=self.conv(F.relu(self.bn(x)))\n",
    "        out=F.avg_pool2d(out,2)\n",
    "        return out\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self,block,nblocks,growth_rate=12,reduction=0.5,num_classes=10):\n",
    "        super(DenseNet,self).__init__()\n",
    "        self.growth_rate=growth_rate\n",
    "        num_planes=2*growth_rate #32\n",
    "        #最初的感知层\n",
    "        self.conv1=nn.Conv2d(3,num_planes,kernel_size=3,padding=1,bias=False)\n",
    "        #第一个DenseBlock\n",
    "        self.dense1=self._make_dense_layers(block,num_planes,nblocks[0])\n",
    "        num_planes+=nblocks[0]*growth_rate\n",
    "        out_planes=int(math.floor(num_planes*reduction))\n",
    "        self.trans1=Transition(num_planes,out_planes)\n",
    "        num_planes=out_planes\n",
    "        #第二个DenseBlock\n",
    "        self.dense2 = self._make_dense_layers(block, num_planes, nblocks[1])\n",
    "        num_planes += nblocks[1]*growth_rate#计算如果不压缩的话的输出\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans2 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "        #第三个DenseBlock\n",
    "        self.dense3 = self._make_dense_layers(block, num_planes, nblocks[2])\n",
    "        num_planes += nblocks[2]*growth_rate\n",
    "        out_planes = int(math.floor(num_planes*reduction))\n",
    "        self.trans3 = Transition(num_planes, out_planes)\n",
    "        num_planes = out_planes\n",
    "        #第四个DenseBlock\n",
    "        self.dense4 = self._make_dense_layers(block, num_planes, nblocks[3])\n",
    "        num_planes += nblocks[3]*growth_rate\n",
    "        #分类层\n",
    "        self.bn=nn.BatchNorm2d(num_planes)\n",
    "        self.linear=nn.Linear(num_planes,num_classes)\n",
    "    \n",
    "    \n",
    "    def _make_dense_layers(self,block,in_planes,nblock):\n",
    "        #block:bottleneck\n",
    "        #nblock代表构建denseblock中有多少bottleneck层\n",
    "        layers=[]\n",
    "        for i in range(nblock):\n",
    "            layers.append(block(in_planes,self.growth_rate))\n",
    "            in_planes+=self.growth_rate\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        out=self.conv1(x)\n",
    "        out=self.trans1(self.dense1(out))\n",
    "        out = self.trans2(self.dense2(out))\n",
    "        out = self.trans3(self.dense3(out))\n",
    "        out = self.dense4(out)\n",
    "        out=F.avg_pool2d(F.relu(self.bn(out)),4)\n",
    "        out=out.view(out.size(0),-1)\n",
    "        out=self.linear(out)\n",
    "        return out\n",
    "\n",
    "def DenseNet121():\n",
    "    return DenseNet(Bottleneck,[6,12,24,16],growth_rate=32)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \n",
    "    expansion = 2\n",
    "\n",
    "    def __init__(self, in_planes, cardinality=32, bottleneck_width=4, stride=1):\n",
    "        super(Block, self).__init__()\n",
    "        group_width = cardinality * bottleneck_width\n",
    "        self.conv1 = nn.Conv2d(in_planes, group_width, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(group_width)\n",
    "        self.conv2 = nn.Conv2d(group_width, group_width, kernel_size=3, stride=stride, padding=1, groups=cardinality, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(group_width)\n",
    "        self.conv3 = nn.Conv2d(group_width, self.expansion*group_width, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(self.expansion*group_width)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*group_width:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*group_width, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*group_width)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = F.relu(self.bn2(self.conv2(out)))\n",
    "        out = self.bn3(self.conv3(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNeXt(nn.Module):\n",
    "    def __init__(self, num_blocks, cardinality, bottleneck_width, num_classes=10):\n",
    "        super(ResNeXt, self).__init__()\n",
    "        self.cardinality = cardinality\n",
    "        self.bottleneck_width = bottleneck_width\n",
    "        self.in_planes = 64\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(num_blocks[0], 1)\n",
    "        self.layer2 = self._make_layer(num_blocks[1], 2)\n",
    "        self.layer3 = self._make_layer(num_blocks[2], 2)\n",
    "        # self.layer4 = self._make_layer(num_blocks[3], 2)\n",
    "        self.linear = nn.Linear(cardinality*bottleneck_width*8, num_classes)\n",
    "\n",
    "    def _make_layer(self, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(Block(self.in_planes, self.cardinality, self.bottleneck_width, stride))\n",
    "            self.in_planes = Block.expansion * self.cardinality * self.bottleneck_width\n",
    "        # Increase bottleneck_width by 2 after each stage.\n",
    "        self.bottleneck_width *= 2\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        # out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "def ResNeXt29_2x64d():\n",
    "    return ResNeXt(num_blocks=[3,3,3], cardinality=2, bottleneck_width=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SqueezeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.init as init\n",
    "class Fire(nn.Module):\n",
    "    def __init__(self,inplanes,s1,e1,e3):\n",
    "        super(Fire,self).__init__()\n",
    "        self.inplanes=inplanes\n",
    "        self.squeeze=nn.Conv2d(inplanes,s1,kernel_size=1)\n",
    "        self.squeeze_activation=nn.ReLU(inplace=True)\n",
    "        self.expand1x1=nn.Conv2d(s1,e1,kernel_size=1)\n",
    "        self.expand1x1_activation=nn.ReLU(inplace=True)\n",
    "        self.expand3x3=nn.Conv2d(s1,e3,kernel_size=True)\n",
    "        self.expand3x3_activation=nn.ReLU(inplace=True)\n",
    "    def forward(self,x):\n",
    "        x=self.squeeze_activation(self.squeeze(x))\n",
    "        return torch.cat([\n",
    "            self.expand1x1_activation(self.expand1x1(x)),\n",
    "            self.expand3x3_activation(self.expand3x3(x))\n",
    "        ],1)\n",
    "    \n",
    "class SqueezeNet(nn.Module):\n",
    "\n",
    "    def __init__(self, version=1.0, num_classes=10):\n",
    "        super(SqueezeNet, self).__init__()\n",
    "        if version not in [1.0, 1.1]:\n",
    "            raise ValueError(\"Unsupported SqueezeNet version {version}:\"\n",
    "                             \"1.0 or 1.1 expected\".format(version=version))\n",
    "        self.num_classes = num_classes\n",
    "        if version == 1.0:\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 96, kernel_size=7, stride=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(96, 16, 64, 64),\n",
    "                Fire(128, 16, 64, 64),\n",
    "                Fire(128, 32, 128, 128),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(256, 32, 128, 128),\n",
    "                Fire(256, 48, 192, 192),\n",
    "                Fire(384, 48, 192, 192),\n",
    "                Fire(384, 64, 256, 256),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(512, 64, 256, 256),\n",
    "            )\n",
    "        else:\n",
    "            self.features = nn.Sequential(\n",
    "                nn.Conv2d(3, 64, kernel_size=3, stride=2),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(64, 16, 64, 64),\n",
    "                Fire(128, 16, 64, 64),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(128, 32, 128, 128),\n",
    "                Fire(256, 32, 128, 128),\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True),\n",
    "                Fire(256, 48, 192, 192),\n",
    "                Fire(384, 48, 192, 192),\n",
    "                Fire(384, 64, 256, 256),\n",
    "                Fire(512, 64, 256, 256),\n",
    "            )\n",
    "        # Final convolution is initialized differently form the rest\n",
    "        final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            #nn.Dropout(p=0.5),\n",
    "            final_conv,\n",
    "            #nn.ReLU(inplace=True),\n",
    "            #nn.AvgPool2d(4, stride=1)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                if m is final_conv:\n",
    "                    init.normal(m.weight.data, mean=0.0, std=0.01)\n",
    "                else:\n",
    "                    init.kaiming_uniform(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x.view(x.size(0), self.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 载入模型并进行计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir='models'\n",
    "\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.makedirs(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=LeNet().to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'lenet_mnist.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=AlexNet().to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'alex_mnist.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=VGGNet11(VGGBlock,nn.MaxPool2d,True).to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'vgg_mnist.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=VGGNet16(VGGBlock,nn.MaxPool2d,True).to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'vgg16_mnist.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=GoogLeNet().to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'googlenet_mnist.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=ResNet18(ResNetLayer,ResNetBlock).to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'resnet18_mnist.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=DenseNet121().to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'densenet121_mnist.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=ResNeXt29_2x64d().to(device)\n",
    "# model_path=os.path.join(model_dir,'resnext_29_2x64d_mnist.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=LeNet().to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'lenet_cifar10.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=ResNet18(ResNetLayer,ResNetBlock).to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "# model_path=os.path.join(model_dir,'resnet18_cifar10.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:73: UserWarning: nn.init.kaiming_uniform is now deprecated in favor of nn.init.kaiming_uniform_.\n",
      "/usr/local/lib/python2.7/dist-packages/ipykernel_launcher.py:71: UserWarning: nn.init.normal is now deprecated in favor of nn.init.normal_.\n"
     ]
    }
   ],
   "source": [
    "model=SqueezeNet(version=1.0).to(device)#将神经网络对象加载到相应的内存或显存中\n",
    "model_path=os.path.join(model_dir,'squeezenet_cifar10.pt')#保存训练好的模型的位置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SqueezeNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2))\n",
       "    (1): ReLU(inplace)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (3): Fire(\n",
       "      (squeeze): Conv2d(96, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(True, True), stride=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (4): Fire(\n",
       "      (squeeze): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(16, 64, kernel_size=(True, True), stride=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (5): Fire(\n",
       "      (squeeze): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(True, True), stride=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (6): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (7): Fire(\n",
       "      (squeeze): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(32, 128, kernel_size=(True, True), stride=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (8): Fire(\n",
       "      (squeeze): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(True, True), stride=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (9): Fire(\n",
       "      (squeeze): Conv2d(384, 48, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(48, 192, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(48, 192, kernel_size=(True, True), stride=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (10): Fire(\n",
       "      (squeeze): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(True, True), stride=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "    (11): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=True)\n",
       "    (12): Fire(\n",
       "      (squeeze): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (squeeze_activation): ReLU(inplace)\n",
       "      (expand1x1): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (expand1x1_activation): ReLU(inplace)\n",
       "      (expand3x3): Conv2d(64, 256, kernel_size=(True, True), stride=(1, 1))\n",
       "      (expand3x3_activation): ReLU(inplace)\n",
       "    )\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Conv2d(512, 10, kernel_size=(1, 1), stride=(1, 1))\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "#criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accu(fx,y):\n",
    "    pred=fx.max(1,keepdim=True)[1]\n",
    "    correct=pred.eq(y.view_as(pred)).sum()\n",
    "    acc=correct.float()/pred.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "#训练一个epoch\n",
    "def train(model,device,iterator,optimizer,criterion):\n",
    "    epoch_loss=0#积累变量\n",
    "    epoch_acc=0#积累变量\n",
    "    model.train()#该函数表示PHASE=Train,自动求导以及求导运算将会被激活\n",
    "    \n",
    "    for (x,y) in iterator:#拿每一个minibatch\n",
    "#         print(x,y)\n",
    "        x=x.to(device)\n",
    "        y=y.to(device)\n",
    "        optimizer.zero_grad()#将所有的梯度变量清零\n",
    "        fx=model(x)#进行inference\n",
    "        loss=criterion(fx,y)#计算train_loss\n",
    "        acc=accu(fx,y)#计算train_acc\n",
    "        loss.backward()#进行bp回算各参数和神经元的梯度\n",
    "        optimizer.step()#统一进行梯度下降的更新\n",
    "        epoch_loss+=loss.item()\n",
    "        epoch_acc+=acc.item()\n",
    "    \n",
    "    #返回平均训练Loss和平均训练Accu\n",
    "    return epoch_loss/len(iterator),epoch_acc/len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "#评测一个验证集，不用梯度下降，只是进行推断和误差计算\n",
    "def evaluate(model,device,iterator,criterion):\n",
    "    epoch_loss=0\n",
    "    epoch_acc=0\n",
    "    model.eval()#PHASE=Eval,不会增加梯度的存储变量和计算单元\n",
    "    with torch.no_grad():\n",
    "        for (x,y) in iterator:\n",
    "            x=x.to(device)\n",
    "        \n",
    "            y=y.to(device)\n",
    "            fx=model(x)\n",
    "            loss=criterion(fx,y)\n",
    "            acc=accu(fx,y)\n",
    "            epoch_loss+=loss.item()\n",
    "            epoch_acc+=acc.item()\n",
    "            \n",
    "    return epoch_loss/len(iterator),epoch_acc/len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss=float('inf')#自动筛选最优模型并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:1|Train Loss:1.41294204359|Train Acc:0.476185191761|Val Loss:1.45674347425|Val Acc:0.455498417722\n",
      "Epoch:2|Train Loss:1.36020674082|Train Acc:0.497447620739|Val Loss:1.37036217391|Val Acc:0.491693037975\n",
      "Epoch:3|Train Loss:1.36073239757|Train Acc:0.496027166193|Val Loss:1.42691243751|Val Acc:0.474485759494\n",
      "Epoch:4|Train Loss:1.35324021615|Train Acc:0.500710227273|Val Loss:1.43550501292|Val Acc:0.464200949367\n",
      "Epoch:5|Train Loss:1.35542793563|Train Acc:0.496493252841|Val Loss:1.39609891704|Val Acc:0.479034810127\n",
      "Epoch:6|Train Loss:1.36374496745|Train Acc:0.499045632102|Val Loss:1.37437524524|Val Acc:0.494462025316\n",
      "Epoch:7|Train Loss:1.34163775287|Train Acc:0.505038174716|Val Loss:1.4011049829|Val Acc:0.479628164557\n",
      "Epoch:8|Train Loss:1.3294310988|Train Acc:0.511563387784|Val Loss:1.38756002203|Val Acc:0.487143987342\n",
      "Epoch:9|Train Loss:1.35666103077|Train Acc:0.498668323864|Val Loss:1.39069735098|Val Acc:0.480617088608\n",
      "Epoch:10|Train Loss:1.33876824311|Train Acc:0.508278586648|Val Loss:1.40429696403|Val Acc:0.485363924051\n",
      "Epoch:11|Train Loss:1.35938200812|Train Acc:0.499977805398|Val Loss:1.52778843988|Val Acc:0.427808544304\n",
      "Epoch:12|Train Loss:1.34782398179|Train Acc:0.503595525568|Val Loss:1.38260732497|Val Acc:0.484375\n",
      "Epoch:13|Train Loss:1.39218115451|Train Acc:0.485595703125|Val Loss:1.38916098619|Val Acc:0.493473101266\n",
      "Epoch:14|Train Loss:1.35990395541|Train Acc:0.501775568182|Val Loss:1.4167746064|Val Acc:0.46875\n",
      "Epoch:15|Train Loss:1.37141473862|Train Acc:0.491876775568|Val Loss:1.38802495335|Val Acc:0.48457278481\n",
      "Epoch:16|Train Loss:1.34614349986|Train Acc:0.50312943892|Val Loss:1.35543046269|Val Acc:0.497231012658\n",
      "Epoch:17|Train Loss:1.33676969506|Train Acc:0.50761274858|Val Loss:1.42122080658|Val Acc:0.48457278481\n",
      "Epoch:18|Train Loss:1.34629220541|Train Acc:0.502352627841|Val Loss:1.33313866884|Val Acc:0.502768987342\n",
      "Epoch:19|Train Loss:1.33159549314|Train Acc:0.51094193892|Val Loss:1.35675240997|Val Acc:0.507120253165\n",
      "Epoch:20|Train Loss:1.3411332556|Train Acc:0.506325461648|Val Loss:1.39149828802|Val Acc:0.480814873418\n",
      "Epoch:21|Train Loss:1.34999497413|Train Acc:0.506036931818|Val Loss:1.39692117745|Val Acc:0.484375\n",
      "Epoch:22|Train Loss:1.36341517139|Train Acc:0.497980291193|Val Loss:1.44437018225|Val Acc:0.466376582278\n",
      "Epoch:23|Train Loss:1.37408573773|Train Acc:0.494806463068|Val Loss:1.51293714137|Val Acc:0.427215189873\n",
      "Epoch:24|Train Loss:1.35048069703|Train Acc:0.501864346591|Val Loss:1.34817079502|Val Acc:0.513647151899\n",
      "Epoch:25|Train Loss:1.33552222784|Train Acc:0.510165127841|Val Loss:1.42430830002|Val Acc:0.46894778481\n",
      "Epoch:26|Train Loss:1.35319300563|Train Acc:0.500177556818|Val Loss:1.348874664|Val Acc:0.506526898734\n",
      "Epoch:27|Train Loss:1.29388946566|Train Acc:0.525745738636|Val Loss:1.33556903767|Val Acc:0.502571202532\n",
      "Epoch:28|Train Loss:1.31277699396|Train Acc:0.517245205966|Val Loss:1.36328204297|Val Acc:0.487935126582\n",
      "Epoch:29|Train Loss:1.30935283962|Train Acc:0.519797585227|Val Loss:1.32502357266|Val Acc:0.510087025316\n",
      "Epoch:30|Train Loss:1.31173366461|Train Acc:0.51396040483|Val Loss:1.36098655794|Val Acc:0.495648734177\n",
      "Epoch:31|Train Loss:1.30527414178|Train Acc:0.520929509943|Val Loss:1.3602001667|Val Acc:0.499011075949\n",
      "Epoch:32|Train Loss:1.3135906877|Train Acc:0.519664417614|Val Loss:1.33614680058|Val Acc:0.504746835443\n",
      "Epoch:33|Train Loss:1.31703725787|Train Acc:0.515380859375|Val Loss:1.35048256796|Val Acc:0.496439873418\n",
      "Epoch:34|Train Loss:1.33632437241|Train Acc:0.5087890625|Val Loss:1.36543526755|Val Acc:0.497428797468\n",
      "Epoch:35|Train Loss:1.3343611595|Train Acc:0.509121981534|Val Loss:1.35502279559|Val Acc:0.494857594937\n",
      "Epoch:36|Train Loss:1.30167096515|Train Acc:0.521062677557|Val Loss:1.34277874307|Val Acc:0.507515822785\n",
      "Epoch:37|Train Loss:1.32427106561|Train Acc:0.511408025568|Val Loss:1.34407500376|Val Acc:0.50019778481\n",
      "Epoch:38|Train Loss:1.3110393152|Train Acc:0.5166015625|Val Loss:1.35963888108|Val Acc:0.504351265823\n",
      "Epoch:39|Train Loss:1.31766496099|Train Acc:0.51396040483|Val Loss:1.35336411603|Val Acc:0.495648734177\n",
      "Epoch:40|Train Loss:1.32441518676|Train Acc:0.515158913352|Val Loss:1.41818772841|Val Acc:0.493868670886\n",
      "Epoch:41|Train Loss:1.35954646529|Train Acc:0.500399502841|Val Loss:1.49342786964|Val Acc:0.445015822785\n",
      "Epoch:42|Train Loss:1.34345863218|Train Acc:0.50419477983|Val Loss:1.33345446255|Val Acc:0.511075949367\n",
      "Epoch:43|Train Loss:1.34975212575|Train Acc:0.500909978693|Val Loss:1.40245267862|Val Acc:0.483386075949\n",
      "Epoch:44|Train Loss:1.32035451586|Train Acc:0.517511541193|Val Loss:1.3312277975|Val Acc:0.501384493671\n",
      "Epoch:45|Train Loss:1.2994563047|Train Acc:0.521994850852|Val Loss:1.35298652438|Val Acc:0.503560126582\n",
      "Epoch:46|Train Loss:1.35021906253|Train Acc:0.502840909091|Val Loss:1.37348167051|Val Acc:0.48496835443\n",
      "Epoch:47|Train Loss:1.29966597437|Train Acc:0.52587890625|Val Loss:1.38799099303|Val Acc:0.485759493671\n",
      "Epoch:48|Train Loss:1.32879022602|Train Acc:0.51151899858|Val Loss:1.36810981926|Val Acc:0.503757911392\n",
      "Epoch:49|Train Loss:1.30647233569|Train Acc:0.52177290483|Val Loss:1.3305206661|Val Acc:0.507911392405\n",
      "Epoch:50|Train Loss:1.27579589349|Train Acc:0.53251509233|Val Loss:1.37080405332|Val Acc:0.492484177215\n",
      "Epoch:51|Train Loss:1.28741213806|Train Acc:0.528497869318|Val Loss:1.33270575427|Val Acc:0.509295886076\n",
      "Epoch:52|Train Loss:1.27575604059|Train Acc:0.534024325284|Val Loss:1.31581816719|Val Acc:0.51582278481\n",
      "Epoch:53|Train Loss:1.28571217524|Train Acc:0.528742009943|Val Loss:1.34091522497|Val Acc:0.507318037975\n",
      "Epoch:54|Train Loss:1.28025766547|Train Acc:0.53144975142|Val Loss:1.34128667735|Val Acc:0.512064873418\n",
      "Epoch:55|Train Loss:1.30277149244|Train Acc:0.524813565341|Val Loss:1.325083169|Val Acc:0.505735759494\n",
      "Epoch:56|Train Loss:1.28527426906|Train Acc:0.528187144886|Val Loss:1.33604736871|Val Acc:0.520569620253\n",
      "Epoch:57|Train Loss:1.28611588309|Train Acc:0.529629794034|Val Loss:1.41144445274|Val Acc:0.485957278481\n",
      "Epoch:58|Train Loss:1.26593855083|Train Acc:0.537220348011|Val Loss:1.34643429291|Val Acc:0.510878164557\n",
      "Epoch:59|Train Loss:1.28097690981|Train Acc:0.530007102273|Val Loss:1.31582061701|Val Acc:0.508504746835\n",
      "Epoch:60|Train Loss:1.26301690377|Train Acc:0.537553267045|Val Loss:1.35080177271|Val Acc:0.501582278481\n",
      "Epoch:61|Train Loss:1.28934746794|Train Acc:0.526455965909|Val Loss:1.30569764029|Val Acc:0.518789556962\n",
      "Epoch:62|Train Loss:1.27689962343|Train Acc:0.532737038352|Val Loss:1.35619869036|Val Acc:0.505142405063\n",
      "Epoch:63|Train Loss:1.27462590655|Train Acc:0.531871448864|Val Loss:1.40887558008|Val Acc:0.493473101266\n",
      "Epoch:64|Train Loss:1.27216922204|Train Acc:0.533025568182|Val Loss:1.31123235859|Val Acc:0.514833860759\n",
      "Epoch:65|Train Loss:1.27696536532|Train Acc:0.532270951705|Val Loss:1.32930281494|Val Acc:0.507911392405\n",
      "Epoch:66|Train Loss:1.32271046979|Train Acc:0.512828480114|Val Loss:1.31692643332|Val Acc:0.513647151899\n",
      "Epoch:67|Train Loss:1.27410693026|Train Acc:0.533558238636|Val Loss:1.36779005618|Val Acc:0.498219936709\n",
      "Epoch:68|Train Loss:1.26938254581|Train Acc:0.533025568182|Val Loss:1.32169865355|Val Acc:0.521954113924\n",
      "Epoch:69|Train Loss:1.28957861645|Train Acc:0.52607865767|Val Loss:1.36207282543|Val Acc:0.499011075949\n",
      "Epoch:70|Train Loss:1.29249475973|Train Acc:0.524569424716|Val Loss:1.32766400787|Val Acc:0.509493670886\n",
      "Epoch:71|Train Loss:1.27108934403|Train Acc:0.531649502841|Val Loss:1.33855398848|Val Acc:0.499208860759\n",
      "Epoch:72|Train Loss:1.29002642733|Train Acc:0.526344992898|Val Loss:1.37525046174|Val Acc:0.490901898734\n",
      "Epoch:73|Train Loss:1.28261736476|Train Acc:0.529141512784|Val Loss:1.34448048812|Val Acc:0.511867088608\n",
      "Epoch:74|Train Loss:1.26865098989|Train Acc:0.533513849432|Val Loss:1.29525596205|Val Acc:0.525514240506\n",
      "Epoch:75|Train Loss:1.24979577992|Train Acc:0.542103160511|Val Loss:1.35431268623|Val Acc:0.511471518987\n",
      "Epoch:76|Train Loss:1.26671206756|Train Acc:0.533247514205|Val Loss:1.36035395046|Val Acc:0.512064873418\n",
      "Epoch:77|Train Loss:1.28859504109|Train Acc:0.528342507102|Val Loss:1.33610233174|Val Acc:0.509493670886\n",
      "Epoch:78|Train Loss:1.29352794842|Train Acc:0.525035511364|Val Loss:1.40403228772|Val Acc:0.471321202532\n",
      "Epoch:79|Train Loss:1.2597785173|Train Acc:0.539972478693|Val Loss:1.33629701711|Val Acc:0.51542721519\n",
      "Epoch:80|Train Loss:1.27710934822|Train Acc:0.53193803267|Val Loss:1.32719142301|Val Acc:0.513053797468\n",
      "Epoch:81|Train Loss:1.28519732488|Train Acc:0.528497869318|Val Loss:1.33831027339|Val Acc:0.511471518987\n",
      "Epoch:82|Train Loss:1.25488522801|Train Acc:0.540083451705|Val Loss:1.30101252885|Val Acc:0.518987341772\n",
      "Epoch:83|Train Loss:1.27254684515|Train Acc:0.53486772017|Val Loss:1.37104600291|Val Acc:0.491099683544\n",
      "Epoch:84|Train Loss:1.28051907552|Train Acc:0.533158735795|Val Loss:1.32024988947|Val Acc:0.510482594937\n",
      "Epoch:85|Train Loss:1.26698812927|Train Acc:0.535999644886|Val Loss:1.37077394241|Val Acc:0.506922468354\n",
      "Epoch:86|Train Loss:1.28799677835|Train Acc:0.527920809659|Val Loss:1.40115530129|Val Acc:0.488330696203\n",
      "Epoch:87|Train Loss:1.25721370141|Train Acc:0.538951526989|Val Loss:1.37132609343|Val Acc:0.495253164557\n",
      "Epoch:88|Train Loss:1.2786979837|Train Acc:0.531960227273|Val Loss:1.34917833986|Val Acc:0.513647151899\n",
      "Epoch:89|Train Loss:1.33024536852|Train Acc:0.512029474432|Val Loss:1.38770409475|Val Acc:0.488132911392\n",
      "Epoch:90|Train Loss:1.27250499329|Train Acc:0.533269708807|Val Loss:1.29337054265|Val Acc:0.53065664557\n",
      "Epoch:91|Train Loss:1.24890640725|Train Acc:0.542547052557|Val Loss:1.30612492486|Val Acc:0.524129746835\n",
      "Epoch:92|Train Loss:1.24501548674|Train Acc:0.543678977273|Val Loss:1.29935372706|Val Acc:0.527096518987\n",
      "Epoch:93|Train Loss:1.24653459738|Train Acc:0.54072709517|Val Loss:1.29366917399|Val Acc:0.523734177215\n",
      "Epoch:94|Train Loss:1.25461132469|Train Acc:0.5390625|Val Loss:1.43050237245|Val Acc:0.482990506329\n",
      "Epoch:95|Train Loss:1.23904909845|Train Acc:0.542524857955|Val Loss:1.34467742337|Val Acc:0.505735759494\n",
      "Epoch:96|Train Loss:1.23246628241|Train Acc:0.546320134943|Val Loss:1.30468704882|Val Acc:0.521360759494\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    train_loss,train_acc=train(model,device,train_iterator,optimizer,criterion)\n",
    "    valid_loss,valid_acc=evaluate(model,device,valid_iterator,criterion)\n",
    "    if valid_loss<best_valid_loss:#当前模型好于历史最好模型\n",
    "        best_valid_loss=valid_loss\n",
    "        torch.save(model.state_dict(),model_path)#模型文件的更新\n",
    "    print('Epoch:{0}|Train Loss:{1}|Train Acc:{2}|Val Loss:{3}|Val Acc:{4}'.format(epoch+1,train_loss,train_acc,valid_loss,valid_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path))#load最好结果到model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss,test_acc=evaluate(model,device,test_iterator,criterion)\n",
    "print('Test Loss:{0}|Test Acc:{1}'.format(test_loss,test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "作业:\n",
    "\n",
    "请大家实现ResNet50/101/152->只需要成功的跑起来即可"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
