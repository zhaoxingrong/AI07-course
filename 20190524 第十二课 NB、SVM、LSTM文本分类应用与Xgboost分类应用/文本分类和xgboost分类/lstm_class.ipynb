{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM 原理展示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lstm展示](lstm.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split #划分train test\n",
    "import multiprocessing #多进程模块\n",
    "import numpy as np #numpy 模块\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec #引入word2vec\n",
    "from gensim.corpora.dictionary import Dictionary #引入字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence \n",
    "from keras.models import Sequential #序列模型\n",
    "from keras.layers import Bidirectional #双向模块\n",
    "from keras.callbacks import EarlyStopping #早停机制\n",
    "from keras.layers.embeddings import Embedding #嵌入层\n",
    "from keras.layers.recurrent import LSTM #循环神经网络\n",
    "from keras.layers.core import Dense, Dropout,Activation #全联接层，dropout防过拟合， 激活层\n",
    "from keras.models import load_model #导入模型层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba #结巴分词\n",
    "import pandas as pd #pandas包，数据表格处理模块\n",
    "import yaml #数据序列化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_chinese(uchar):\n",
    "#   判断一个unicode是否是汉字\n",
    "    if uchar >= u'\\u4e00' and uchar <= u'\\u9fa5':\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_df():\n",
    "#   获取正负例dataframe 和 停用词list\n",
    "    neg_df=pd.read_table('neg.txt',sep='\\t',names=['label','chat'])\n",
    "    neg_df['label'] = 0\n",
    "    pos_df=pd.read_table('pos.txt',sep='\\t',names=['label','chat'])\n",
    "    stop_list=list(set(pd.read_table('chinese-stopword.txt',sep='\\t',names=['stop'])['stop']))\n",
    "    return neg_df,pos_df,stop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xdata_label(method_select):\n",
    "#   词频特征的向量化\n",
    "    neg_df,pos_df,stop_list = get_data_df()\n",
    "    neg_df['cut']=neg_df['chat'].map(str).apply(method_select)\n",
    "    del neg_df['chat']\n",
    "    pos_df['cut']=pos_df['chat'].map(str).apply(method_select)\n",
    "    del pos_df['chat']\n",
    "#     neg_df,pos_df = get_cut_word(method_select)\n",
    "    xdata=[]\n",
    "    ylabel=[]\n",
    "    neg_pos = pd.concat([neg_df,pos_df])\n",
    "    for i in range(len(neg_pos)):\n",
    "        ixdata = neg_pos.iloc[i,1]\n",
    "        ilabel = neg_pos.iloc[i,0]\n",
    "        if ixdata!='':\n",
    "            xdata.append(ixdata)\n",
    "            ylabel.append(ilabel)\n",
    "    return xdata,ylabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jieba_getdata(str_str):\n",
    "#   结巴切词\n",
    "    stopwords=list(set(pd.read_table('chinese-stopword.txt',sep='\\t',names=['stop'])['stop']))\n",
    "    cut_list=jieba.lcut(str_str)\n",
    "    cut_list=[iword for iword in cut_list if iword not in stopwords]\n",
    "    cut_list=[iword for iword in cut_list if is_chinese(iword) is True]\n",
    "    cut_str=' '.join(cut_list)\n",
    "    return cut_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dictionaries(p_model):\n",
    "    # 根据现有的word2vec模型构建词向量\n",
    "    gensim_dict = Dictionary()\n",
    "    gensim_dict.doc2bow(p_model.wv.vocab.keys(), allow_update=True) #doc2bow, 计算机只认识数字\n",
    "    w2indx = {v: k + 1 for k, v in gensim_dict.items()}  # 词语的索引，从1开始编号,频数较小的词语索引为0\n",
    "    w2vec = {word: p_model[word] for word in w2indx.keys()}  # 词语的词向量\n",
    "    return w2indx, w2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dataset(xdata,w2indx):\n",
    "#   把word变为整数\n",
    "    data=[]\n",
    "    for sentence in xdata:\n",
    "        new_txt = []\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                new_txt.append(w2indx[word])\n",
    "            except:\n",
    "                new_txt.append(0)\n",
    "        data.append(new_txt)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_transform(string,p_model):\n",
    "    words=jieba.lcut(string)\n",
    "    words=np.array(words).reshape(1,-1)\n",
    "    _,_,combined=create_dictionaries(p_model,words)\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/ipykernel/__main__.py:6: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "p_model = gensim.models.Word2Vec.load('w2v_input.model') #导入w2v\n",
    "w2indx, w2vec = create_dictionaries(p_model) #构建字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata,ylabel=get_xdata_label(jieba_getdata) # 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = parse_dataset(xdata,w2indx) #word转id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= sequence.pad_sequences(data, maxlen=100) #padding 填充"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9897, 100)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_data, ylabel, test_size=0.2) #划分数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [EarlyStopping(monitor='accuracy', patience=2, verbose=1, mode='max')]\n",
    "\n",
    "# '''\n",
    "# 如果epoch数量太少，网络有可能发生欠拟合（即对于定型数据的学习不够充分）；\n",
    "# 如果epoch数量太多，则有可能发生过拟合（即网络对定型数据中的“噪声”而非信号拟合）。\n",
    "\n",
    "# 早停法旨在解决epoch数量需要手动设置的问题。它也可以被视为一种能够避免网络发生过拟合的正则化方法（与L1/L2权重衰减和丢弃法类似）。\n",
    "\n",
    "# 目的还是解决过拟合！\n",
    "# 早停法背后的原理其实不难理解：\n",
    "\n",
    "# 将数据分为定型集和测试集\n",
    "# 每个epoch结束后（或每N个epoch后）：\n",
    "# 用测试集评估网络性能\n",
    "# 如果网络性能表现优于此前最好的模型：保存当前这一epoch的网络副本\n",
    "# 将测试性能最优的模型作为最终网络模型\n",
    "# keras.callbacks.EarlyStopping(monitor='val_loss', patience=0, verbose=0, mode='auto')\n",
    "\n",
    "# monitor：需要监视的值\n",
    "\n",
    "# verbose：信息展示模式，0或1\n",
    "\n",
    "# save_best_only：当设置为True时，将只保存在验证集上性能最好的模型\n",
    "\n",
    "# mode：‘auto’，‘min’，‘max’之一，在save_best_only=True时决定性能最佳模型的评判准则，例如，当监测值为val_acc时，模式应为max，当检测值为val_loss时，模式应为min。在auto模式下，评价准则由被监测值的名字自动推断。\n",
    "\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/ipykernel/__main__.py:16: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(units=50, activation=\"relu\")`\n"
     ]
    }
   ],
   "source": [
    "vocab_dim = 128\n",
    "n_symbols = len(w2indx)+1 # 词典数+1\n",
    "batch_size = 64\n",
    "n_epoch = 3\n",
    "\n",
    "embedding_weights = np.zeros((n_symbols, 128))#索引为0的词语，词向量全为0\n",
    "for word, index in w2indx.items():#从索引为1的词语开始，对每个词语对应其词向量\n",
    "    embedding_weights[index, :] = w2vec[word]\n",
    "\n",
    "model = Sequential()  # or Graph or whatever\n",
    "model.add(Embedding(output_dim=vocab_dim,\n",
    "                    input_dim=n_symbols,\n",
    "                    mask_zero=True,\n",
    "                    weights =[embedding_weights],\n",
    "                    input_length=100))  # Adding Input Length\n",
    "model.add(Bidirectional(LSTM(output_dim=50, activation='relu')))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1)) # 一个神经元\n",
    "model.add(Activation('sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12028"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, 100, 128)          1539584   \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, 100)               71600     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 101       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,611,285\n",
      "Trainable params: 1,611,285\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the Model...\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/ipykernel/__main__.py:7: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7917 samples, validate on 1980 samples\n",
      "Epoch 1/3\n",
      "7917/7917 [==============================] - 68s 9ms/step - loss: 0.5939 - acc: 0.7077 - val_loss: 0.5763 - val_acc: 0.7323\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.4/lib/python3.4/site-packages/keras/callbacks.py:569: RuntimeWarning: Early stopping conditioned on metric `accuracy` which is not available. Available metrics are: val_loss,acc,val_acc,loss\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7917/7917 [==============================] - 68s 9ms/step - loss: 0.5263 - acc: 0.7515 - val_loss: 0.5361 - val_acc: 0.8025\n",
      "Epoch 3/3\n",
      "7917/7917 [==============================] - 64s 8ms/step - loss: 0.4754 - acc: 0.7850 - val_loss: 0.4383 - val_acc: 0.7889\n",
      "Evaluate...\n",
      "1980/1980 [==============================] - 5s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "print ('Compiling the Model...')\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',metrics=['accuracy'])\n",
    "\n",
    "print (\"Train...\")\n",
    "model.fit(x_train, np.array(y_train), batch_size=batch_size, nb_epoch=n_epoch,\n",
    "          verbose=1, validation_data=(x_test, np.array(y_test)), callbacks=my_callbacks)\n",
    "\n",
    "print (\"Evaluate...\")\n",
    "score = model.evaluate(x_test, np.array(y_test),\n",
    "                            batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = '标准间太差房间还不如3星的而且设施非常陈旧.建议酒店把老的标准间从新改善'\n",
    "# string = '我住的行政房是全新的，有五星级标准*v*。只可惜楼顶的中餐厅在停业装修，我挺喜欢这个餐厅的，正宗广东菜，希望下次来时复业了。似乎酒店花了很多精力在装修更新。'\n",
    "str_list = jieba_getdata(string)\n",
    "new_txt = []\n",
    "for word in str_list:\n",
    "    try:\n",
    "        new_txt.append(w2indx[word])\n",
    "    except:\n",
    "        new_txt.append(0)\n",
    "\n",
    "pre_text = sequence.pad_sequences([new_txt], maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,  7381,  2476, 11393,     0,  4297,\n",
       "            0,  5040,     0,  5891, 11393,     0,  6889,     0, 10294,\n",
       "         6695,     0, 11452,  6802,     0,  5312,     0,     0, 11156,\n",
       "         5269,     0,  7381,  2476, 11393,     0,  6651,     0,  6513,\n",
       "            0]], dtype=int32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "标准间太差房间还不如3星的而且设施非常陈旧.建议酒店把老的标准间从新改善 positive\n"
     ]
    }
   ],
   "source": [
    "result=model.predict_proba(pre_text)\n",
    "if result[0][0]>0.5:\n",
    "    print (string,'positive')\n",
    "else:\n",
    "    print (string,'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.57297057]], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## text CNN ###########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![textcnn](textcnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import Input, Model\n",
    "from keras.layers import Embedding, Dense, Conv1D, GlobalMaxPooling1D, Concatenate, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "n_symbols = len(w2indx)+1 # 词典数+1\n",
    "input = Input((maxlen,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding(n_symbols, 128, weights = [embedding_weights],input_length=maxlen)(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = []\n",
    "for kernel_size in [3, 4, 5]:\n",
    "    c = Conv1D(64, kernel_size, activation='relu')(embedding) #卷积\n",
    "    c = GlobalMaxPooling1D()(c)\n",
    "    convs.append(c)\n",
    "x = Concatenate()(convs)\n",
    "\n",
    "output = Dense(1, activation='relu')(x)\n",
    "model = Model(inputs=input, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 100, 128)     1539584     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 98, 64)       24640       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 97, 64)       32832       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 96, 64)       41024       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 64)           0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_5 (GlobalM (None, 64)           0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_6 (GlobalM (None, 64)           0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 192)          0           global_max_pooling1d_4[0][0]     \n",
      "                                                                 global_max_pooling1d_5[0][0]     \n",
      "                                                                 global_max_pooling1d_6[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            193         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 1,638,273\n",
      "Trainable params: 1,638,273\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7917 samples, validate on 1980 samples\n",
      "Epoch 1/3\n",
      "7917/7917 [==============================] - 28s 4ms/step - loss: 1.2929 - acc: 0.6290 - val_loss: 0.6953 - val_acc: 0.7202\n",
      "Epoch 2/3\n",
      "7917/7917 [==============================] - 31s 4ms/step - loss: 0.5913 - acc: 0.7672 - val_loss: 0.5692 - val_acc: 0.7621\n",
      "Epoch 3/3\n",
      "7917/7917 [==============================] - 30s 4ms/step - loss: 0.5444 - acc: 0.7873 - val_loss: 0.5015 - val_acc: 0.7990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x10c028dd8>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_acc', patience=3, mode='max')\n",
    "model.fit(x_train, np.array(y_train),\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          callbacks=[early_stopping],\n",
    "          validation_data=(x_test, np.array(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string = '标准间太差房间还不如3星的而且设施非常陈旧.建议酒店把老的标准间从新改善'\n",
    "string = '我住的行政房是全新的，有五星级标准*v*。只可惜楼顶的中餐厅在停业装修，我挺喜欢这个餐厅的，正宗广东菜，希望下次来时复业了。似乎酒店花了很多精力在装修更新。'\n",
    "str_list = jieba_getdata(string)\n",
    "new_txt = []\n",
    "for word in str_list:\n",
    "    try:\n",
    "        new_txt.append(w2indx[word])\n",
    "    except:\n",
    "        new_txt.append(0)\n",
    "\n",
    "pre_text = sequence.pad_sequences([new_txt], maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "我住的行政房是全新的，有五星级标准*v*。只可惜楼顶的中餐厅在停业装修，我挺喜欢这个餐厅的，正宗广东菜，希望下次来时复业了。似乎酒店花了很多精力在装修更新。 positive\n"
     ]
    }
   ],
   "source": [
    "result=model.predict(pre_text)\n",
    "if result[0][0]>0.5:\n",
    "    print (string,'positive')\n",
    "else:\n",
    "    print (string,'negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
